{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from egsage import EGraphSage\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Dataset, DataLoader, Batch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from itertools import combinations\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "# For reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv', index_col=0)\n",
    "raw_data_matrix = data.values\n",
    "\n",
    "label_data = pd.read_csv('label.csv')\n",
    "label_data = label_data.values[:,1:]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_ratio = 0.6\n",
    "valid_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "# First, split the data into train and remaining sets\n",
    "train_data, remaining_data, train_labels, remaining_labels = train_test_split(raw_data_matrix, label_data, test_size=1-train_ratio)\n",
    "\n",
    "# Then, split the remaining data into validation and test sets\n",
    "test_ratio_adjusted = test_ratio / (valid_ratio + test_ratio)  # Adjust the test ratio\n",
    "valid_data, test_data, valid_labels, test_labels = train_test_split(remaining_data, remaining_labels, test_size=test_ratio_adjusted)\n",
    "\n",
    "\n",
    "def generate_graph(raw_data_matrix):\n",
    "    num_samples, num_nodes = raw_data_matrix.shape\n",
    "\n",
    "    # Calculate proportions based on the entire dataset\n",
    "    proportions = np.mean(raw_data_matrix, axis=0)\n",
    "\n",
    "    matrix_x = np.zeros((num_nodes, num_nodes))\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            logic = np.logical_and(raw_data_matrix.T[i] == 1, raw_data_matrix.T[j] == 1)\n",
    "            if np.any(logic):\n",
    "                matrix_x[i, j] = 1\n",
    "\n",
    "    # Generate edge list for fully connected graph\n",
    "    edge_list = list(combinations(range(num_nodes), 2))\n",
    "\n",
    "    return proportions, matrix_x, edge_list\n",
    "\n",
    "\n",
    "def create_data_object(raw_data_batch, proportions, matrix_x, edge_list):\n",
    "    # Create graph edges table\n",
    "    graph_edges = np.array(edge_list)\n",
    "    #node_labels = torch.tensor(raw_data_batch, dtype=torch.float) # convert to tensor here\n",
    "    graph_edges_dict = {\n",
    "        'src': graph_edges[:, 0].tolist(),\n",
    "        'dst': graph_edges[:, 1].tolist(),\n",
    "        'weight': np.round(matrix_x[graph_edges[:, 0], graph_edges[:, 1]], 3).tolist()\n",
    "    }\n",
    "    graph_edges_df = pd.DataFrame(graph_edges_dict)\n",
    "\n",
    "    edge_index = torch.tensor(graph_edges_df[['src', 'dst']].values.T, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(graph_edges_df['weight'].tolist(), dtype=torch.float)\n",
    "\n",
    "    data = Data(x=None, edge_index=edge_index, edge_attr=edge_attr, y=None)\n",
    "    data = data.to(device)  # Move data object to device once\n",
    "    \n",
    "    return data\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, raw_data, labels, train_proportions, matrix_x, edge_list, graph_data):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.labels = labels\n",
    "        self.proportions = torch.tensor(train_proportions, dtype=torch.float).to(device) # move to device here\n",
    "        self.matrix_x = matrix_x\n",
    "        self.edge_list = edge_list\n",
    "        self.graph_data = graph_data\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        raw_data = self.raw_data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Update the node embeddings and labels of the graph data object\n",
    "        node_labels = torch.tensor(raw_data, dtype=torch.float).unsqueeze(0).to(self.proportions.device) # move to the same device as proportions\n",
    "        node_labels = torch.where(node_labels == 1, self.proportions, 1 - self.proportions)\n",
    "        self.graph_data.x = node_labels\n",
    "\n",
    "        processed_label = torch.tensor(label, dtype=torch.float)\n",
    "        self.graph_data.y = processed_label\n",
    "\n",
    "        return self.graph_data\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Define MLP model\n",
    "class MultiTaskMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiTaskMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5), \n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()  # Add Sigmoid activation function\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "# proportions, matrix_x, edge_list = generate_graph(raw_data_matrix)\n",
    "# np.save('proportions.npy',proportions)\n",
    "# np.save('matrix_x.npy',matrix_x)\n",
    "proportions, matrix_x, edge_list = generate_graph(raw_data_matrix)\n",
    "#edge_list = list(combinations(range(raw_data_matrix.shape[1]), 2))\n",
    "graph_data = create_data_object(None, proportions, matrix_x, edge_list)\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = GraphDataset(train_data, train_labels, proportions, matrix_x, edge_list, graph_data)\n",
    "valid_dataset = GraphDataset(valid_data, valid_labels, proportions, matrix_x, edge_list, graph_data)\n",
    "test_dataset  = GraphDataset(test_data,  test_labels,  proportions, matrix_x, edge_list, graph_data)\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define GNN model\n",
    "gnn_input_dim = 1\n",
    "gnn_output_dim = 1\n",
    "\n",
    "gnn_model = EGraphSage(gnn_input_dim, gnn_output_dim, edge_channels=1, activation='relu', edge_mode=1, normalize_emb=True, aggr='mean').to(device)\n",
    "#(node_in_dim,node_out_dim,edge_dim,activation,edge_mode,normalize_emb, aggr)\n",
    "\n",
    "# Define Binary MLP models for each task\n",
    "mlp_input_dim = 692\n",
    "mlp_hidden_dim = 64\n",
    "mlp_model = MultiTaskMLP(mlp_input_dim, mlp_hidden_dim, 4).to(device)\n",
    "\n",
    "# Define loss functions for each task\n",
    "criterion_1 = nn.BCELoss()\n",
    "criterion_2 = nn.BCELoss()\n",
    "criterion_3 = nn.BCELoss()\n",
    "criterion_4 = nn.BCELoss()\n",
    "\n",
    "# Define optimizer for all models\n",
    "optimizer = optim.Adam(\n",
    "    list(gnn_model.parameters()) +\n",
    "    list(mlp_model.parameters()),\n",
    "    lr=0.00001\n",
    ")\n",
    "\n",
    "# ... Other imports and definitions ...\n",
    "num_epochs = 300\n",
    "best_model_params = None\n",
    "patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Initialize the best validation loss to a large number\n",
    "best_val_loss = float('inf')\n",
    "model_dir = './models'  # Directory to save model parameters\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "train_loss = []\n",
    "val_loss =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model parameters and evaluate on the test set\n",
    "gnn_model.load_state_dict(torch.load(os.path.join(model_dir, 'best_gnn_model.pth')))\n",
    "mlp_model.load_state_dict(torch.load(os.path.join(model_dir, 'best_mlp_model.pth')))\n",
    "\n",
    "gnn_model.eval()\n",
    "mlp_model.eval()\n",
    "\n",
    "list_output = []\n",
    "list_y = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_loader), desc='Test', unit='batch') as pbar:\n",
    "        for data in test_loader:\n",
    "            data.to(device)\n",
    "\n",
    "            x_test, edge_index_test, edge_attr_test, y_test = data.x, data.edge_index, data.edge_attr, data.y\n",
    "            x_test = x_test.squeeze().unsqueeze(-1)\n",
    "            gnn_output_test = gnn_model(x_test, edge_attr_test, edge_index_test)\n",
    "            mlp_output = mlp_model(gnn_output_test.view(-1))\n",
    "            \n",
    "            list_output.append(mlp_output.cpu().detach())\n",
    "            list_y.append(y_test.cpu().detach())\n",
    "\n",
    "            pbar.update(1)\n",
    "    # 重置进度条\n",
    "    pbar.close()\n",
    "    \n",
    "    \n",
    "true_labels_np = np.array([tensor.numpy() for tensor in list_y])\n",
    "model_outputs_np = np.array([tensor.numpy() for tensor in list_output])\n",
    "pred_labels = np.round(model_outputs_np) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_np = true_labels_np.T\n",
    "model_outputs_np = model_outputs_np.T\n",
    "pred_labels = pred_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def metrics(true_labels,predicted_labels):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    # 计算 TP、FP 和 FN\n",
    "    for true_label, predicted_label in zip(true_labels, predicted_labels):\n",
    "        if true_label == 1 and predicted_label == 1:\n",
    "            TP += 1\n",
    "        elif true_label == 0 and predicted_label == 1:\n",
    "            FP += 1\n",
    "        elif true_label == 1 and predicted_label == 0:\n",
    "            FN += 1\n",
    "\n",
    "    # 计算 Precision 和 Recall\n",
    "    precision = TP / (TP + FP+1e-9)\n",
    "    recall = TP / (TP + FN +1e-9)\n",
    "\n",
    "    return precision, recall\n",
    "    \n",
    "all_f1 = []\n",
    "total_prc = []\n",
    "total_auc = []\n",
    "total_recall = []\n",
    "total_precision = []\n",
    "drug_count = []\n",
    "\n",
    "for index in range(pred_labels.shape[0]):\n",
    "    drug_count.append(np.sum(pred_labels[index]==1))\n",
    "for i in range(pred_labels.shape[0]):\n",
    "#     if np.all(true_labels_np.T[i] !=0):\n",
    "    precision,recall = metrics(true_labels_np[i],pred_labels[i])\n",
    "    total_recall.append(recall)\n",
    "    total_precision.append(precision)\n",
    "    prc = average_precision_score(true_labels_np[i],model_outputs_np[i], average='macro')\n",
    "    total_prc.append(prc)\n",
    "    f1 = f1_score(true_labels_np[i], pred_labels[i], average='macro')\n",
    "    all_f1.append(f1)\n",
    "    auc = roc_auc_score(true_labels_np[i],model_outputs_np[i])\n",
    "    total_auc.append(auc)\n",
    "    \n",
    "\n",
    "def jaccard_sim(a, b):\n",
    "    unions = len(set(a).union(set(b)))\n",
    "    intersections = len(set(a).intersection(set(b)))\n",
    "    return intersections / unions\n",
    "\n",
    "drug_count = []\n",
    "total_jaccard=[]\n",
    "for index in range(pred_labels.T.shape[0]):\n",
    "    drug_count.append(np.sum(pred_labels.T[index]==1))\n",
    "for i in range(pred_labels.T.shape[0]):\n",
    "    if np.all(true_labels_np.T[i] !=0):\n",
    "    #     if np.all(true_labels_np[i] != 0):\n",
    "#         if np.all(true_labels_np[i] != 1):\n",
    "#         total_auc.append(0)\n",
    "#         continue\n",
    "        total_jaccard.append(jaccard_sim(np.where(pred_labels.T[i] == 1)[0], np.where(true_labels_np.T[i] == 1)[0]))\n",
    "\n",
    "print(\"总的drug counts:\",  np.mean(drug_count))\n",
    "print(\"总的jaccard:\",  np.mean(total_jaccard))\n",
    "print(\"总的PRAUC:\",  np.mean(total_prc))\n",
    "print(\"总的AUROC:\",  np.mean(total_auc))\n",
    "print(\"总的F1:\",  np.mean(all_f1))\n",
    "print(\"总的recall:\",  np.mean(total_recall))\n",
    "print(\"总的precision:\",  np.mean(total_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parkconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
